# Performance Modeling of Industrial DNN Applications
Cloud-based Deep Neural Network (DNN) applications that make latency-sensitive inference are becoming an indispensable part of Industry 4.0. Due to the multi-tenancy and resource heterogeneity, both inherent to the cloud computing environments, the inference time of DNN-based applications are stochastic. Such stochasticity, if not captured, can potentially lead to low Quality of Service (QoS) or even a disaster in critical sectors, such as Oil and Gas industry. To make Industry 4.0 robust, solution architects and researchers need to understand the behavior of DNN-based applications and capture the stochasticity exists in their inference times. Accordingly, in this study, we provide a descriptive analysis of the inference time from two perspectives. First, we perform an application-centric analysis and statistically model the execution time of four categorically different DNN applications on both Amazon and Chameleon clouds. Second, we take a resource-centric approach and analyze a rate-based metric in form of Million Instruction Per Second (MIPS) for heterogeneous machines in the cloud. This non-parametric modeling, achieved via Jackknife and Bootstrap re-sampling methods, provides the confidence interval of MIPS for heterogeneous cloud machines. The findings of this research can be helpful for researchers and cloud solution architects to develop solutions that are robust against the stochastic nature of the inference time of DNN applications in the cloud and can offer a higher QoS to their users and avoid unintended outcomes. Therefore, this repository is the benchmark of inference execution times of four different categories of DNN-based applications. The applications are given below -

1. Fire Detection (Abbreviated as Fire)
2. Human Activity Recognition (Abbreviated as HAR)
3. Oil Spill Detection (Abbreviated as Oil)
4. Acuistic Impedence Estimation (Abbreviated as AIE)

We deployed the pretrained models of the applications in the cloud platforms and run the inference operations with test data to capture the time of inference. For each application, we run the inference operations thirty times and consider it as sample data to model the performance. The detailed performance modeling of the applications are provided in the paper named "Performance Modeling of Deep Neural Network Applications on Heterogeneous Cloud Resources" that is submitted in IGSC 2020.

The description of the folders are given below:
1. Applications: Four different DNN applications source code. Specially, the inference part. For pretrained model's weight and original git repository, there is a file named "link.txt" where the hyperlinks are saved. Additionally, for inference data the same file should be utilized to get the data. 
2. AWS: The inference execution time traces of DNN applications in five different vitual machine instances of AWS EC2. Along with the inference execution times, we also store the testing and resampling tools with their results for the verification purpose.
3. Chameleon: The inference execution time traces of DNN applications running in four different instances of Chameleon are stored in this folder. The name of the folders are the type of the instances that are utilized in this work.

